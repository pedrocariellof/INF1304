version: '3.8'

##
# Docker Compose para:
# "Balanceamento de Carga e Failover com Kafka em Clusters Docker"
# Mini-mundo: Sistema de Monitoramento de Sensores
##
services:

  # -------------------------
  # Kafka Broker / Controller 1 (KRaft)
  # -------------------------
  kafka1:
    container_name: kafka1
    build: ./kafka
    ports:
      - "9092:9092"    # internal/container listener (container-to-container)
      - "19092:19092"  # controller listener (container-to-container)
      - "29092:29092"  # EXTERNAL listener (host -> this broker)  (opcional p/ testes)
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      # Escuta em 3 listeners: PLAINTEXT (intra-container), CONTROLLER, EXTERNAL (host)
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:19092,EXTERNAL://0.0.0.0:29092
      # Advertised: como outros containers/hosts devem se conectar
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092,CONTROLLER://kafka1:19092,EXTERNAL://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      # KRaft quorum voters: cada nó com seu port CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:19092,2@kafka2:19093,3@kafka3:19094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      # Replication / durabilidade (ajuste se recursos forem limitados)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      # Partições padrão (pode ser ignorado; preferir criação via kafka-topics)
      KAFKA_NUM_PARTITIONS: 6
    volumes:
      - kafka1_logs:/var/lib/kafka/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "kafka-broker-api-versions --bootstrap-server localhost:9092 >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - int-network

  # -------------------------
  # Kafka Broker / Controller 2 (KRaft)
  # -------------------------
  kafka2:
    container_name: kafka2
    build: ./kafka
    ports:
      - "9093:9093"
      - "19093:19093"
      - "29093:29093"
    environment:
      KAFKA_NODE_ID: 2
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093,CONTROLLER://0.0.0.0:19093,EXTERNAL://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka2:9093,CONTROLLER://kafka2:19093,EXTERNAL://localhost:29093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:19092,2@kafka2:19093,3@kafka3:19094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 6
    volumes:
      - kafka2_logs:/var/lib/kafka/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "kafka-broker-api-versions --bootstrap-server localhost:9093 >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - int-network

  # -------------------------
  # Kafka Broker / Controller 3 (KRaft)
  # -------------------------
  kafka3:
    container_name: kafka3
    build: ./kafka
    ports:
      - "9094:9094"
      - "19094:19094"
      - "29094:29094"
    environment:
      KAFKA_NODE_ID: 3
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094,CONTROLLER://0.0.0.0:19094,EXTERNAL://0.0.0.0:29094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka3:9094,CONTROLLER://kafka3:19094,EXTERNAL://localhost:29094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka1:19092,2@kafka2:19093,3@kafka3:19094
      KAFKA_LOG_DIRS: /var/lib/kafka/data
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 6
    volumes:
      - kafka3_logs:/var/lib/kafka/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "kafka-broker-api-versions --bootstrap-server localhost:9094 >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
    networks:
      - int-network

  # -------------------------
  # Kafdrop (opcional) - UI para visualizar tópicos/partições
  # -------------------------
  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    environment:
      KAFKA_BROKERCONNECT: "kafka1:9092,kafka2:9093,kafka3:9094"
    ports:
      - "9000:9000"
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    networks:
      - int-network
      - ext-network
    restart: unless-stopped

  # -------------------------
  # Producer service (simulador de sensores) - instancia 0
  # - Build espera encontrar ./producer-service/Dockerfile
  # - Use variáveis para evitar hard-coded
  # -------------------------
  producer-service0:
    container_name: producer_service0
    build: ./producer-service
    ports:
      - "8080:8080"   # se o producer expõe endpoint HTTP para controle/metrics
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    environment:
      # conectar via nomes/ports internas entre containers
      KAFKA_BROKERS: "kafka1:9092,kafka2:9093,kafka3:9094"
      KAFKA_TOPIC: "dados-sensores"
      SENSOR_ID: "sensor-00"
      SENSOR_INTERVAL: "5"   # segundos
    restart: unless-stopped
    networks:
      - int-network
      - ext-network

  # Producer instancia 1 (outra máquina/sensor aggregator)
  producer-service1:
    container_name: producer_service1
    build: ./producer-service
    ports:
      - "8180:8080"
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    environment:
      KAFKA_BROKERS: "kafka1:9092,kafka2:9093,kafka3:9094"
      KAFKA_TOPIC: "dados-sensores"
      SENSOR_ID: "sensor-01"
      SENSOR_INTERVAL: "5"
    restart: unless-stopped
    networks:
      - int-network
      - ext-network

  # -------------------------
  # Consumer service (processador) - instancia 0
  # - Recomendo Java Spring Boot com spring-kafka; use env vars no application.properties
  # -------------------------
  consumer-service0:
    build: ./consumer-service
    container_name: consumer_service0
    ports:
      - "8081:8080"
    environment:
      KAFKA_BROKERS: "kafka1:9092,kafka2:9093,kafka3:9094"
      KAFKA_TOPIC: "dados-sensores"
      KAFKA_CONSUMER_GROUP: "processadores-sensores"
      DB_PATH: "/app/data/alerts.db"   
    volumes:
      - ./consumer-service/data:/app/data    
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    networks:
      - int-network
      - ext-network

  consumer-service1:
    build: ./consumer-service
    container_name: consumer_service1
    ports:
      - "8181:8080"
    environment:
      KAFKA_BROKERS: "kafka1:9092,kafka2:9093,kafka3:9094"
      KAFKA_TOPIC: "dados-sensores"
      KAFKA_CONSUMER_GROUP: "processadores-sensores"
      DB_PATH: "/app/data/alerts.db"   
    volumes:
      - ./consumer-service/data:/app/data
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    networks:
      - int-network
      - ext-network


  # -------------------------
  # Frontend simples (opcional) - serve diretório ./frontend
  # -------------------------
  frontend:
    container_name: frontend1
    image: nginx:alpine
    volumes:
      - ./frontend:/usr/share/nginx/html:ro
    ports:
      - "8000:80"
    networks:
      - ext-network
    restart: unless-stopped


# -------------------------
# redes e volumes
# -------------------------
networks:
  ext-network:
    driver: bridge
  int-network:
    driver: bridge
    internal: true

volumes:
  kafka1_logs:
  kafka2_logs:
  kafka3_logs:
  consumer0_data:
  consumer1_data: